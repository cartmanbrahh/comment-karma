{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting reddit comment karma | Web mining \n",
    "\n",
    "## Part I | Feature Engineering\n",
    "\n",
    "### Ankita BHATTACHARYA\n",
    "##### M2, Statistics & Econometrics, TSE\n",
    "\n",
    "The goal of this term project is to predict the score or **ups** of a reddit comment. In this first notebook, we shall be using text (notably Soft Clustering) and network mining methods to choose our features that will go into predicting the **ups** in the second notebook of this series. \n",
    "\n",
    "The first notebook contains the detailed methods of feature generation. After performing a brief exploratory analysis we extract three types of features : \n",
    "- Content-based features\n",
    "    - Features based on the comments  : stylometric characteristics\n",
    "        1. Length of the comment in words\n",
    "        2. Count words beginning with capital letters\n",
    "        3. Count number of exclamations\n",
    "        4. Question marks\n",
    "        5. Punctuation count\n",
    "        6. Number of 'I's used in the comment\n",
    "        7. Number of urls\n",
    "        8. Associated subreddits\n",
    "    - I added these two features to improve the model:\n",
    "        9. Lexical Diversity\n",
    "        10. Number of times comment references another comment\n",
    " \n",
    " \n",
    "- Text mining using soft clustering : LDA, 20 features\n",
    "        \n",
    "- Time based features\n",
    "   1. Time difference between OP and other comments\n",
    "   2. Hour of the comment\n",
    "   3. Day of the comment\n",
    "   4. Time between the OP's first comment & the others by thread\n",
    "\n",
    "- Network based features :Structural characteristics\n",
    "   1. Reply to first comment : Earlier comments get more traction, and so do the replies to these comments \n",
    "   2. Thread popularity : calculating the length of the thread, or the number of unique authors in the link id\n",
    "\n",
    "**Comment-Comment features : How responding to which comment gives you a bigger score**\n",
    "   3. Comment depth : Calculating the comments made in response to a thread\n",
    "   4. Number of neighbours : Calculating the neighbours for each thread\n",
    "\n",
    "**Author-Author features : Interaction between users and its impact on the score**\n",
    "  5. Degree centrality\n",
    "  6. Betweenness centrality\n",
    "  7. Eigenvector centrality \n",
    "\n",
    "8. Score by parent_id :Obtaining the score of comments by the parent_id or the score of the post to which the comments are a reply to.\n",
    "9. Number of comments made by author\n",
    "\n",
    "\n",
    "**Kernel : Python 3**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline\n",
    "\n",
    "- Loading requisite libraries \n",
    "- Data Importation\n",
    "- Exploratory analysis of the data\n",
    "- Feature engineering\n",
    "- Text Mining\n",
    "    - Preprocessing\n",
    "    - Soft Clustering : Latent Dirichlet Allocation (TF matrix)\n",
    "- Network Mining\n",
    "    - Creating links \n",
    "    - Calculating measures of centrality\n",
    "- Calculating time based features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading requisite libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install ntlk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Universal\n",
    "import re\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "import gc \n",
    "\n",
    "# Text mining\n",
    "import nltk\n",
    "from nltk import Text\n",
    "from nltk import tokenize\n",
    "from nltk.tokenize import word_tokenize  \n",
    "from nltk.tokenize import sent_tokenize \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from autocorrect import Speller\n",
    "\n",
    "# Term Document Matrix\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "\n",
    "# Exporting results into pickle\n",
    "import pickle\n",
    "\n",
    "# Network mining\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the data\n",
    "The data comments.csv must be stored in the same directory as the notebook\n",
    "\n",
    "Data definition :\n",
    "- created_utc: The time of creation in UTC epoch-second format. \n",
    "- ups: The score or upvotes-downvotes received by the comment.  \n",
    "- subreddit_id: Here we only see comments coming from AskReddit\n",
    "- link_id: ID of the link this comment is in\n",
    "- name: Fullname of comment, e.g.\\t1_c3v7f8u\"\n",
    "- subreddit: Name of the subreddit\n",
    "- id: The ID of each comment \n",
    "- author: The reddit name of the poster\n",
    "- body: The raw comment as posted on reddit\n",
    "- parent_id: ID of the thing this comment is a reply to, either the link or comment in it\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4234970, 10)\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"comments_students.csv\") \n",
    "print(data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_utc</th>\n",
       "      <th>ups</th>\n",
       "      <th>subreddit_id</th>\n",
       "      <th>link_id</th>\n",
       "      <th>name</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>id</th>\n",
       "      <th>author</th>\n",
       "      <th>body</th>\n",
       "      <th>parent_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1430438400</td>\n",
       "      <td>3.0</td>\n",
       "      <td>t5_2qh1i</td>\n",
       "      <td>t3_34f9rh</td>\n",
       "      <td>t1_cqug90j</td>\n",
       "      <td>AskReddit</td>\n",
       "      <td>cqug90j</td>\n",
       "      <td>jesse9o3</td>\n",
       "      <td>No one has a European accent either  because i...</td>\n",
       "      <td>t1_cqug2sr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1430438400</td>\n",
       "      <td>3.0</td>\n",
       "      <td>t5_2qh1i</td>\n",
       "      <td>t3_34fvry</td>\n",
       "      <td>t1_cqug90k</td>\n",
       "      <td>AskReddit</td>\n",
       "      <td>cqug90k</td>\n",
       "      <td>beltfedshooter</td>\n",
       "      <td>That the kid ..reminds me of Kevin.   so sad :-(</td>\n",
       "      <td>t3_34fvry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1430438400</td>\n",
       "      <td>5.0</td>\n",
       "      <td>t5_2qh1i</td>\n",
       "      <td>t3_34ffo5</td>\n",
       "      <td>t1_cqug90z</td>\n",
       "      <td>AskReddit</td>\n",
       "      <td>cqug90z</td>\n",
       "      <td>InterimFatGuy</td>\n",
       "      <td>NSFL</td>\n",
       "      <td>t1_cqu80zb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1430438401</td>\n",
       "      <td>1.0</td>\n",
       "      <td>t5_2qh1i</td>\n",
       "      <td>t3_34aqsn</td>\n",
       "      <td>t1_cqug91c</td>\n",
       "      <td>AskReddit</td>\n",
       "      <td>cqug91c</td>\n",
       "      <td>JuanTutrego</td>\n",
       "      <td>I'm a guy and I had no idea this was a thing g...</td>\n",
       "      <td>t1_cqtdj4m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1430438401</td>\n",
       "      <td>101.0</td>\n",
       "      <td>t5_2qh1i</td>\n",
       "      <td>t3_34f9rh</td>\n",
       "      <td>t1_cqug91e</td>\n",
       "      <td>AskReddit</td>\n",
       "      <td>cqug91e</td>\n",
       "      <td>dcblackbelt</td>\n",
       "      <td>Mid twenties male rocking skinny jeans/pants, ...</td>\n",
       "      <td>t1_cquc4rc</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   created_utc    ups subreddit_id    link_id        name  subreddit       id  \\\n",
       "0   1430438400    3.0     t5_2qh1i  t3_34f9rh  t1_cqug90j  AskReddit  cqug90j   \n",
       "1   1430438400    3.0     t5_2qh1i  t3_34fvry  t1_cqug90k  AskReddit  cqug90k   \n",
       "2   1430438400    5.0     t5_2qh1i  t3_34ffo5  t1_cqug90z  AskReddit  cqug90z   \n",
       "3   1430438401    1.0     t5_2qh1i  t3_34aqsn  t1_cqug91c  AskReddit  cqug91c   \n",
       "4   1430438401  101.0     t5_2qh1i  t3_34f9rh  t1_cqug91e  AskReddit  cqug91e   \n",
       "\n",
       "           author                                               body  \\\n",
       "0        jesse9o3  No one has a European accent either  because i...   \n",
       "1  beltfedshooter   That the kid ..reminds me of Kevin.   so sad :-(   \n",
       "2   InterimFatGuy                                               NSFL   \n",
       "3     JuanTutrego  I'm a guy and I had no idea this was a thing g...   \n",
       "4     dcblackbelt  Mid twenties male rocking skinny jeans/pants, ...   \n",
       "\n",
       "    parent_id  \n",
       "0  t1_cqug2sr  \n",
       "1   t3_34fvry  \n",
       "2  t1_cqu80zb  \n",
       "3  t1_cqtdj4m  \n",
       "4  t1_cquc4rc  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preview the first 5 lines of the loaded data \n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking for missing values\n",
    "We see that 24% of the dataset has missing upvotes  - this will constitute our test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Inspored by : Twitter Sentiment analysis, Kurtis Pykes. \n",
    "# github : https://github.com/kurtispykes/twitter-sentiment-analysis/blob/master/notebook/03_kpy_data_exploration.ipynb\n",
    "\n",
    "\n",
    "# Visualising the missing values % \n",
    "missing_values = dict(zip([col for col in data.columns if col != \"label\"],\n",
    "                          [round(100*data[col].isnull().sum()/len(data), 2) for col in data.columns\n",
    "                           if col != \"label\"]))\n",
    "missing_values_df = pd.DataFrame(missing_values, index=[0])\n",
    "missing_values_df = missing_values_df.melt(var_name= \"columns\", value_name= \"percentage\")\n",
    "\n",
    "# plotting missing values chart\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.set(style=\"whitegrid\", color_codes=True)\n",
    "seaborn_plot = sns.barplot(x=\"columns\", y=\"percentage\", data=missing_values_df)\n",
    "for p in seaborn_plot.patches:\n",
    "    seaborn_plot.annotate(format(p.get_height(), '.2f'), (p.get_x() + p.get_width() / 2., p.get_height()), ha = 'center',\n",
    "                   va = 'center', xytext = (0, 9), textcoords = 'offset points')\n",
    "plt.title(\"% of Missing Values in Columns\", size=20, weight=\"bold\")\n",
    "plt.xlabel(\"Columns\", size=14, weight=\"bold\")\n",
    "plt.ylabel(\"%\", size=14, weight=\"bold\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting into train and test sets\n",
    "We perform the data cleaning on the training set from hereon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_utc</th>\n",
       "      <th>ups</th>\n",
       "      <th>subreddit_id</th>\n",
       "      <th>link_id</th>\n",
       "      <th>name</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>id</th>\n",
       "      <th>author</th>\n",
       "      <th>body</th>\n",
       "      <th>parent_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1430438400</td>\n",
       "      <td>3.0</td>\n",
       "      <td>t5_2qh1i</td>\n",
       "      <td>t3_34f9rh</td>\n",
       "      <td>t1_cqug90j</td>\n",
       "      <td>AskReddit</td>\n",
       "      <td>cqug90j</td>\n",
       "      <td>jesse9o3</td>\n",
       "      <td>No one has a European accent either  because i...</td>\n",
       "      <td>t1_cqug2sr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1430438400</td>\n",
       "      <td>3.0</td>\n",
       "      <td>t5_2qh1i</td>\n",
       "      <td>t3_34fvry</td>\n",
       "      <td>t1_cqug90k</td>\n",
       "      <td>AskReddit</td>\n",
       "      <td>cqug90k</td>\n",
       "      <td>beltfedshooter</td>\n",
       "      <td>That the kid ..reminds me of Kevin.   so sad :-(</td>\n",
       "      <td>t3_34fvry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1430438400</td>\n",
       "      <td>5.0</td>\n",
       "      <td>t5_2qh1i</td>\n",
       "      <td>t3_34ffo5</td>\n",
       "      <td>t1_cqug90z</td>\n",
       "      <td>AskReddit</td>\n",
       "      <td>cqug90z</td>\n",
       "      <td>InterimFatGuy</td>\n",
       "      <td>NSFL</td>\n",
       "      <td>t1_cqu80zb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1430438401</td>\n",
       "      <td>1.0</td>\n",
       "      <td>t5_2qh1i</td>\n",
       "      <td>t3_34aqsn</td>\n",
       "      <td>t1_cqug91c</td>\n",
       "      <td>AskReddit</td>\n",
       "      <td>cqug91c</td>\n",
       "      <td>JuanTutrego</td>\n",
       "      <td>I'm a guy and I had no idea this was a thing g...</td>\n",
       "      <td>t1_cqtdj4m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1430438401</td>\n",
       "      <td>101.0</td>\n",
       "      <td>t5_2qh1i</td>\n",
       "      <td>t3_34f9rh</td>\n",
       "      <td>t1_cqug91e</td>\n",
       "      <td>AskReddit</td>\n",
       "      <td>cqug91e</td>\n",
       "      <td>dcblackbelt</td>\n",
       "      <td>Mid twenties male rocking skinny jeans/pants, ...</td>\n",
       "      <td>t1_cquc4rc</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   created_utc    ups subreddit_id    link_id        name  subreddit       id  \\\n",
       "0   1430438400    3.0     t5_2qh1i  t3_34f9rh  t1_cqug90j  AskReddit  cqug90j   \n",
       "1   1430438400    3.0     t5_2qh1i  t3_34fvry  t1_cqug90k  AskReddit  cqug90k   \n",
       "2   1430438400    5.0     t5_2qh1i  t3_34ffo5  t1_cqug90z  AskReddit  cqug90z   \n",
       "3   1430438401    1.0     t5_2qh1i  t3_34aqsn  t1_cqug91c  AskReddit  cqug91c   \n",
       "4   1430438401  101.0     t5_2qh1i  t3_34f9rh  t1_cqug91e  AskReddit  cqug91e   \n",
       "\n",
       "           author                                               body  \\\n",
       "0        jesse9o3  No one has a European accent either  because i...   \n",
       "1  beltfedshooter   That the kid ..reminds me of Kevin.   so sad :-(   \n",
       "2   InterimFatGuy                                               NSFL   \n",
       "3     JuanTutrego  I'm a guy and I had no idea this was a thing g...   \n",
       "4     dcblackbelt  Mid twenties male rocking skinny jeans/pants, ...   \n",
       "\n",
       "    parent_id  \n",
       "0  t1_cqug2sr  \n",
       "1   t3_34fvry  \n",
       "2  t1_cqu80zb  \n",
       "3  t1_cqtdj4m  \n",
       "4  t1_cquc4rc  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = data[data['ups'].notnull()]\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_utc</th>\n",
       "      <th>ups</th>\n",
       "      <th>subreddit_id</th>\n",
       "      <th>link_id</th>\n",
       "      <th>name</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>id</th>\n",
       "      <th>author</th>\n",
       "      <th>body</th>\n",
       "      <th>parent_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3218512</th>\n",
       "      <td>1432512001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>t5_2qh1i</td>\n",
       "      <td>t3_37566r</td>\n",
       "      <td>t1_crjs2zv</td>\n",
       "      <td>AskReddit</td>\n",
       "      <td>crjs2zv</td>\n",
       "      <td>Jakeable</td>\n",
       "      <td>Hi DE04435, your submission has been removed f...</td>\n",
       "      <td>t3_37566r</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3218513</th>\n",
       "      <td>1432512001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>t5_2qh1i</td>\n",
       "      <td>t3_3754y5</td>\n",
       "      <td>t1_crjs2zx</td>\n",
       "      <td>AskReddit</td>\n",
       "      <td>crjs2zx</td>\n",
       "      <td>icameforthecookies</td>\n",
       "      <td>Fancí</td>\n",
       "      <td>t3_3754y5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3218514</th>\n",
       "      <td>1432512003</td>\n",
       "      <td>NaN</td>\n",
       "      <td>t5_2qh1i</td>\n",
       "      <td>t3_374uoa</td>\n",
       "      <td>t1_crjs30r</td>\n",
       "      <td>AskReddit</td>\n",
       "      <td>crjs30r</td>\n",
       "      <td>thebattlefish</td>\n",
       "      <td>And where can I get some of these hand panties?</td>\n",
       "      <td>t1_crjpooq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3218515</th>\n",
       "      <td>1432512003</td>\n",
       "      <td>NaN</td>\n",
       "      <td>t5_2qh1i</td>\n",
       "      <td>t3_37573q</td>\n",
       "      <td>t1_crjs312</td>\n",
       "      <td>AskReddit</td>\n",
       "      <td>crjs312</td>\n",
       "      <td>stephenjash</td>\n",
       "      <td>Join the military. I have a successful career,...</td>\n",
       "      <td>t3_37573q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3218516</th>\n",
       "      <td>1432512004</td>\n",
       "      <td>NaN</td>\n",
       "      <td>t5_2qh1i</td>\n",
       "      <td>t3_3735sz</td>\n",
       "      <td>t1_crjs31m</td>\n",
       "      <td>AskReddit</td>\n",
       "      <td>crjs31m</td>\n",
       "      <td>MoorgunFreeman</td>\n",
       "      <td>The most horrifying thing I've seen in this th...</td>\n",
       "      <td>t1_crjao8x</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         created_utc  ups subreddit_id    link_id        name  subreddit  \\\n",
       "3218512   1432512001  NaN     t5_2qh1i  t3_37566r  t1_crjs2zv  AskReddit   \n",
       "3218513   1432512001  NaN     t5_2qh1i  t3_3754y5  t1_crjs2zx  AskReddit   \n",
       "3218514   1432512003  NaN     t5_2qh1i  t3_374uoa  t1_crjs30r  AskReddit   \n",
       "3218515   1432512003  NaN     t5_2qh1i  t3_37573q  t1_crjs312  AskReddit   \n",
       "3218516   1432512004  NaN     t5_2qh1i  t3_3735sz  t1_crjs31m  AskReddit   \n",
       "\n",
       "              id              author  \\\n",
       "3218512  crjs2zv            Jakeable   \n",
       "3218513  crjs2zx  icameforthecookies   \n",
       "3218514  crjs30r       thebattlefish   \n",
       "3218515  crjs312         stephenjash   \n",
       "3218516  crjs31m      MoorgunFreeman   \n",
       "\n",
       "                                                      body   parent_id  \n",
       "3218512  Hi DE04435, your submission has been removed f...   t3_37566r  \n",
       "3218513                                              Fancí   t3_3754y5  \n",
       "3218514    And where can I get some of these hand panties?  t1_crjpooq  \n",
       "3218515  Join the military. I have a successful career,...   t3_37573q  \n",
       "3218516  The most horrifying thing I've seen in this th...  t1_crjao8x  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = data[data['ups'].isnull()]\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further cleaning - Training Set\n",
    "Comments on reddit that have already been removed do not contribute to prediction. We shall remove those that say \"removed\". This is only on the training set as all the data needs to be preserved for the test set (Kaggle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_tot = len(train_df)\n",
    "train_df = train_df[train_df.body != '']\n",
    "train_df = train_df[train_df.body != '[removed]']\n",
    "train_df.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4234958, 10)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reconcatenating\n",
    "df = pd.concat([train_df, test_df], ignore_index=True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the datatypes of the df dataset. We will need to convert 'body' from object to string "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['body'] = df['body'].astype('str')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "### A. Content based features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Length of the comment in words : studying the relationship between comment length and upvotes/downvotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['num_words'] = df.body.apply(lambda x: len(str(x).split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Features A.2 to A.5 will extract features relating to a common question in text mining : Is punctuation important ? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Count words beginning with capital letters :  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['words_caps'] = df.body.apply(lambda x: len(re.findall(r'[A-Z]', str(x))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Count number of exclamations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['num_exclamation_marks'] = df['body'].str.count('!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Question marks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['num_question_marks'] = df['body'].str.count(\"\\?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Count punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['num_punctuation'] = df['body'].str.count(\".|,|;|'|:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Number of I's : How important is subjectivity to upvotes ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['num_subject'] = df['body'].str.count(\"I|I've|I'm|I'd|i've|i'm|i'd| i \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Count the number of urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = r'(https?://\\S+)'\n",
    "df['urlcount'] = df.body.apply(lambda x: re.findall(url, str(x))).str.len()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Associated sub reddits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sub_reddits'] = df.body.apply(lambda x: len(re.findall(r\"/r/([^\\s/]+)\", str(x)))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Lexical diversity\n",
    "Ref :  O'Reilly 'Natural Language Processing and Python, page 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lexical_diversity(text):\n",
    "     return len(set(text)) / len(text) if len(text) !=0 else 0\n",
    "    \n",
    "# df['lexicalDiversity'] = df['body'].apply(lexical_diversity)\n",
    "df['lexicalDiversity'] = [lexical_diversity(text) for text in df['body']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. Referencing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for '&gt;'\n",
    "df['is_referencing'] = df.body.str.contains('&gt;').astype(int) \n",
    "# We should also remove this reference \n",
    "df['body'] = df.body.apply(lambda x: str(x).replace('&gt;', ''))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us look at these content based features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_utc</th>\n",
       "      <th>ups</th>\n",
       "      <th>subreddit_id</th>\n",
       "      <th>link_id</th>\n",
       "      <th>name</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>id</th>\n",
       "      <th>author</th>\n",
       "      <th>body</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>...</th>\n",
       "      <th>thread_pop</th>\n",
       "      <th>comment_depth</th>\n",
       "      <th>count_nb</th>\n",
       "      <th>parent_score</th>\n",
       "      <th>nb_com_author</th>\n",
       "      <th>degree_centrality</th>\n",
       "      <th>betweenness_centrality</th>\n",
       "      <th>eigenvector_centrality</th>\n",
       "      <th>is_referencing</th>\n",
       "      <th>lexicalDiversity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1430438400</td>\n",
       "      <td>3.0</td>\n",
       "      <td>t5_2qh1i</td>\n",
       "      <td>t3_34f9rh</td>\n",
       "      <td>t1_cqug90j</td>\n",
       "      <td>AskReddit</td>\n",
       "      <td>cqug90j</td>\n",
       "      <td>jesse9o3</td>\n",
       "      <td>No one has a European accent either  because i...</td>\n",
       "      <td>t1_cqug2sr</td>\n",
       "      <td>...</td>\n",
       "      <td>2786</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10.0</td>\n",
       "      <td>54</td>\n",
       "      <td>0.000141</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>1.358437e-04</td>\n",
       "      <td>0</td>\n",
       "      <td>0.193277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1430438400</td>\n",
       "      <td>3.0</td>\n",
       "      <td>t5_2qh1i</td>\n",
       "      <td>t3_34fvry</td>\n",
       "      <td>t1_cqug90k</td>\n",
       "      <td>AskReddit</td>\n",
       "      <td>cqug90k</td>\n",
       "      <td>beltfedshooter</td>\n",
       "      <td>That the kid ..reminds me of Kevin.   so sad :-(</td>\n",
       "      <td>t3_34fvry</td>\n",
       "      <td>...</td>\n",
       "      <td>12349</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10.0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.249299e-08</td>\n",
       "      <td>0</td>\n",
       "      <td>0.437500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1430438400</td>\n",
       "      <td>5.0</td>\n",
       "      <td>t5_2qh1i</td>\n",
       "      <td>t3_34ffo5</td>\n",
       "      <td>t1_cqug90z</td>\n",
       "      <td>AskReddit</td>\n",
       "      <td>cqug90z</td>\n",
       "      <td>InterimFatGuy</td>\n",
       "      <td>NSFL</td>\n",
       "      <td>t1_cqu80zb</td>\n",
       "      <td>...</td>\n",
       "      <td>7247</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10.0</td>\n",
       "      <td>57</td>\n",
       "      <td>0.000160</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>4.572286e-05</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1430438401</td>\n",
       "      <td>1.0</td>\n",
       "      <td>t5_2qh1i</td>\n",
       "      <td>t3_34aqsn</td>\n",
       "      <td>t1_cqug91c</td>\n",
       "      <td>AskReddit</td>\n",
       "      <td>cqug91c</td>\n",
       "      <td>JuanTutrego</td>\n",
       "      <td>I'm a guy and I had no idea this was a thing g...</td>\n",
       "      <td>t1_cqtdj4m</td>\n",
       "      <td>...</td>\n",
       "      <td>1466</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10.0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1430438401</td>\n",
       "      <td>101.0</td>\n",
       "      <td>t5_2qh1i</td>\n",
       "      <td>t3_34f9rh</td>\n",
       "      <td>t1_cqug91e</td>\n",
       "      <td>AskReddit</td>\n",
       "      <td>cqug91e</td>\n",
       "      <td>dcblackbelt</td>\n",
       "      <td>Mid twenties male rocking skinny jeans/pants, ...</td>\n",
       "      <td>t1_cquc4rc</td>\n",
       "      <td>...</td>\n",
       "      <td>2786</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.880685e-08</td>\n",
       "      <td>0</td>\n",
       "      <td>0.132780</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   created_utc    ups subreddit_id    link_id        name  subreddit       id  \\\n",
       "0   1430438400    3.0     t5_2qh1i  t3_34f9rh  t1_cqug90j  AskReddit  cqug90j   \n",
       "1   1430438400    3.0     t5_2qh1i  t3_34fvry  t1_cqug90k  AskReddit  cqug90k   \n",
       "2   1430438400    5.0     t5_2qh1i  t3_34ffo5  t1_cqug90z  AskReddit  cqug90z   \n",
       "3   1430438401    1.0     t5_2qh1i  t3_34aqsn  t1_cqug91c  AskReddit  cqug91c   \n",
       "4   1430438401  101.0     t5_2qh1i  t3_34f9rh  t1_cqug91e  AskReddit  cqug91e   \n",
       "\n",
       "           author                                               body  \\\n",
       "0        jesse9o3  No one has a European accent either  because i...   \n",
       "1  beltfedshooter   That the kid ..reminds me of Kevin.   so sad :-(   \n",
       "2   InterimFatGuy                                               NSFL   \n",
       "3     JuanTutrego  I'm a guy and I had no idea this was a thing g...   \n",
       "4     dcblackbelt  Mid twenties male rocking skinny jeans/pants, ...   \n",
       "\n",
       "    parent_id  ...  thread_pop  comment_depth  count_nb  parent_score  \\\n",
       "0  t1_cqug2sr  ...        2786              1         1          10.0   \n",
       "1   t3_34fvry  ...       12349              1         1          10.0   \n",
       "2  t1_cqu80zb  ...        7247              1         1          10.0   \n",
       "3  t1_cqtdj4m  ...        1466              1         1          10.0   \n",
       "4  t1_cquc4rc  ...        2786              1         2          10.0   \n",
       "\n",
       "   nb_com_author  degree_centrality  betweenness_centrality  \\\n",
       "0             54           0.000141                0.000023   \n",
       "1              5           0.000007                0.000000   \n",
       "2             57           0.000160                0.000040   \n",
       "3              5           0.000000                0.000000   \n",
       "4              2           0.000005                0.000000   \n",
       "\n",
       "   eigenvector_centrality is_referencing  lexicalDiversity  \n",
       "0            1.358437e-04              0          0.193277  \n",
       "1            1.249299e-08              0          0.437500  \n",
       "2            4.572286e-05              0          1.000000  \n",
       "3            0.000000e+00              0          0.333333  \n",
       "4            2.880685e-08              0          0.132780  \n",
       "\n",
       "[5 rows x 35 columns]"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Mining : Preprocessing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our corpus is massive and to reduce the time for pre-processing, we ought to remove punctuations, special characters and single letters. The idea is therefore to pre-process the data which will then be fed to the vectorizer function. \n",
    "The pre-processing shall include the following steps \n",
    "\n",
    "- Cleaning the text : converting to lower text, removing the punctuations, URLS, special characters etc. \n",
    "- Tokenization\n",
    "- Removing the stop words\n",
    "- Stemming : Eliminating affixes (circumfixes, suffixes, prefixes, infixes) from a word in order to obtain a word stem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\", '\\r']\n"
     ]
    }
   ],
   "source": [
    "# Let us first remove the stop words\n",
    "stop_words = stopwords.words('english')\n",
    "# Removing the subreddit sign\n",
    "stop_words.extend([\"\\r\"])\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # Converting to lower case\n",
    "    text.lower()\n",
    "    # Remove urls\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', text, flags=re.MULTILINE)\n",
    "    # Remove user @ references and '#' from text\n",
    "    text = re.sub(r'\\@\\w+|\\#','', text)\n",
    "    # Remove punctuations\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    # Remove all single characters\n",
    "    text = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', text)\n",
    "    # Remove single characters from the start\n",
    "    text = re.sub(r'\\^[a-zA-Z]\\s+', ' ', text)\n",
    "    # Remove characters of 3 characters\n",
    "    text = re.sub(r'\\b\\w{1,3}\\b', '', text)\n",
    "    # Tokenizing the corpus\n",
    "    text_tokens = word_tokenize(text)\n",
    "    # Remove stopwords\n",
    "    filtered_words = [w for w in text_tokens if not w in stop_words]\n",
    "    # Stemming the corpus\n",
    "    ps = PorterStemmer()\n",
    "    stemmed_words = [ps.stem(w) for w in filtered_words]\n",
    "    \n",
    "    return \" \".join(stemmed_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_preprocessed = df['body'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'european accent either doesnt exist there accent europ european accent'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_preprocessed[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that there are some  spelling errors and inspired by https://rustyonrampage.github.io/text-mining/2017/11/28/spelling-correction-with-python-and-nltk.html I implemented the spelling correction. The idea is first to shorten words like reallllly into really. This however might not be sufficient in correcting all words. We shall then implement the spellchecker using **Speller** from the **autocorrect** library "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'really'"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def reduce_lengthening(text):\n",
    "    pattern = re.compile(r\"(.)\\1{2,}\")\n",
    "    reduced = pattern.sub(r\"\\1\\1\", text)\n",
    "    return \"\".join(reduced)\n",
    "\n",
    "reduce_lengthening(\"reallllllly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = re.compile(r\"(.)\\1{2,}\")\n",
    "red_text = [pattern.sub(r\"\\1\\1\", text) for text in text_preprocessed]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I tried the spellchecker using parallel processing, but it took too long and my computer hung several times. \n",
    "# spell = Speller(lang = 'en')\n",
    "# final_preprocessed = [spell(text) for text in red_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('preprocessed.pkl', 'rb') as f:\n",
    "    text_preprocessed = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Soft Clustering :  Features extracted using LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building the term frequency which shall be applied on the LDA model : \n",
    "* Here we remove words that appear in over 20% of the comments\n",
    "* Remove words appearing in less than 20 comments\n",
    "* The maximum number of words kept per comment are 25.\n",
    "* Keep words having more than 3 characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting the tf features for LDA matrix ...\n",
      "Proceed to next cell.\n"
     ]
    }
   ],
   "source": [
    "print(\"Extracting the tf features for LDA matrix ...\")\n",
    "tf_vectorizer = CountVectorizer(max_df = 0.5, min_df = 20,\n",
    "                                max_features = 20, \n",
    "                                token_pattern='[a-zA-Z0-9]{3,}', # char count >3\n",
    "                                stop_words = 'english')\n",
    "\n",
    "tf = tf_vectorizer.fit_transform(text_preprocessed)\n",
    "print(\"Proceed to next cell.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4.00000000e-02 1.63999250e-01 4.00000000e-02 4.00000000e-02\n",
      "  4.00000000e-02 4.00000000e-02 4.00000000e-02 5.03955507e+05\n",
      "  4.00000000e-02 4.02253499e-02 4.00000000e-02 1.56696544e-01\n",
      "  4.00000000e-02 4.00000000e-02 4.00000000e-02 4.00000000e-02\n",
      "  4.00000000e-02 4.00000000e-02 4.00000000e-02 4.00000000e-02\n",
      "  4.11376298e-02 4.00000079e-02 4.18130524e-02 4.00000000e-02\n",
      "  4.00000000e-02]\n",
      " [4.00000000e-02 1.34057119e-01 4.00000000e-02 4.00000000e-02\n",
      "  4.00000000e-02 2.15304064e-01 4.12788646e-02 7.30530063e+00\n",
      "  4.00000000e-02 8.02167641e-02 4.00025437e-02 4.00000854e-02\n",
      "  4.00000000e-02 4.00000000e-02 4.00000000e-02 4.00000000e-02\n",
      "  1.51136479e-01 4.00000000e-02 4.00375946e-02 4.11702057e-02\n",
      "  4.00000000e-02 2.44213552e+01 1.44513585e-01 2.48349221e+01\n",
      "  2.06708615e+05]\n",
      " [4.00000000e-02 4.00980767e-02 4.00000000e-02 4.00000000e-02\n",
      "  1.52652239e+05 4.00000000e-02 4.00000378e-02 4.10777515e-02\n",
      "  4.00000000e-02 4.00000000e-02 4.00000000e-02 4.00001728e-02\n",
      "  4.00000000e-02 4.00000000e-02 4.00000000e-02 4.00000000e-02\n",
      "  4.00000000e-02 4.00000000e-02 1.07364016e+05 4.00000000e-02\n",
      "  4.00000000e-02 4.00000000e-02 4.00000000e-02 4.00000000e-02\n",
      "  4.00000000e-02]\n",
      " [4.00000000e-02 1.94442959e+03 4.61697556e-02 4.00023432e-02\n",
      "  3.43708189e+04 1.88006558e+00 3.63891648e+03 1.67685065e+04\n",
      "  7.41064383e-02 7.92357754e+01 4.00000001e-02 6.24664570e+00\n",
      "  4.00767159e-02 4.00000000e-02 4.00000000e-02 4.00000000e-02\n",
      "  2.37240836e-01 4.85556831e-01 4.00000000e-02 2.84080070e-01\n",
      "  3.88071180e-01 8.31134314e+03 1.17124277e+04 8.56831040e-02\n",
      "  1.04737970e+01]\n",
      " [4.00000000e-02 9.04129988e+01 4.00000000e-02 4.00000000e-02\n",
      "  4.00000000e-02 4.01575267e-02 2.64446396e+05 4.14804224e+00\n",
      "  4.00000000e-02 5.47163225e-02 4.00000000e-02 5.41446669e+01\n",
      "  4.00000143e-02 4.00000000e-02 4.00000000e-02 4.00000000e-02\n",
      "  4.00498244e-02 4.00000004e-02 4.00000000e-02 4.03032115e-02\n",
      "  4.00000000e-02 5.56605211e-02 3.70109263e-01 4.00000000e-02\n",
      "  4.00000000e-02]\n",
      " [4.00000000e-02 1.29890539e+01 6.84057172e-01 4.00000002e-02\n",
      "  4.00000000e-02 4.00019639e-02 5.91452216e-01 1.33819560e+01\n",
      "  5.61953798e-02 4.34093524e-02 4.00000000e-02 1.07603330e-01\n",
      "  1.28671165e+05 4.00000000e-02 4.00000000e-02 4.00000000e-02\n",
      "  4.20389063e-02 4.01488183e-02 4.00000158e-02 1.01951065e+00\n",
      "  1.71740216e+00 4.24991191e-01 3.96572714e+00 3.38896688e-01\n",
      "  4.00000000e-02]\n",
      " [4.00000000e-02 8.22223757e+03 1.48767312e+01 4.00000000e-02\n",
      "  4.00000000e-02 4.00000000e-02 4.00000000e-02 9.35716061e+01\n",
      "  7.09020670e-02 4.00000000e-02 4.53577954e+03 4.00000000e-02\n",
      "  4.00000000e-02 1.19394012e+05 9.65341206e+04 4.57365128e+04\n",
      "  4.00000000e-02 5.43139091e+01 9.40786800e+00 4.00000000e-02\n",
      "  4.00000000e-02 4.00000000e-02 3.09057632e+01 4.00000000e-02\n",
      "  4.00000000e-02]\n",
      " [4.00000000e-02 4.37647553e+03 2.50152146e-01 4.00000000e-02\n",
      "  4.00000000e-02 2.00619021e+00 3.17055343e+03 6.51512285e+03\n",
      "  4.00000029e-02 1.38377190e-01 4.00000000e-02 2.63510180e+00\n",
      "  4.97224497e+03 4.00000000e-02 4.00000000e-02 4.00000000e-02\n",
      "  1.98034178e+03 5.53871026e+01 7.72122102e-01 3.74103831e+00\n",
      "  1.01731594e+00 8.43409161e-01 1.96871231e+00 5.10272831e-02\n",
      "  4.00000000e-02]\n",
      " [4.00000000e-02 5.78022866e-02 1.23020039e+01 4.00000000e-02\n",
      "  4.00000000e-02 4.00000005e-02 4.00000000e-02 2.91043896e+02\n",
      "  2.25400240e+00 4.00000000e-02 1.16153564e-01 4.00000000e-02\n",
      "  4.00000000e-02 4.00053852e-02 4.00000995e-02 2.69707193e-01\n",
      "  4.00000000e-02 1.58548152e+05 8.34466353e+00 4.00000000e-02\n",
      "  4.00000000e-02 4.00007115e-02 7.18944595e+03 4.00000000e-02\n",
      "  4.00000000e-02]\n",
      " [4.00000000e-02 6.59164958e-02 4.00000000e-02 4.00000000e-02\n",
      "  4.00000000e-02 1.97731333e+05 1.14982627e-01 1.33954912e+02\n",
      "  4.00000000e-02 8.22792738e-02 4.00000001e-02 4.76612268e+00\n",
      "  4.00000000e-02 4.00000000e-02 4.00000000e-02 4.00000000e-02\n",
      "  2.35950415e+00 4.01287036e-02 1.35169686e-01 6.27935434e+01\n",
      "  4.00000010e-02 6.10037179e+00 5.20602102e-02 4.15383486e-02\n",
      "  4.00000000e-02]\n",
      " [4.00000000e-02 2.44151351e-01 4.00000000e-02 4.00000000e-02\n",
      "  4.00000000e-02 6.34043480e-02 4.02109945e-02 3.53293733e+00\n",
      "  4.00000000e-02 2.69542807e+05 4.00000000e-02 5.50366793e+00\n",
      "  4.17580812e-02 4.00000000e-02 4.00000000e-02 4.00000000e-02\n",
      "  8.51213656e-01 4.00000000e-02 4.00008846e-02 4.00000000e-02\n",
      "  4.15448976e-02 5.90369577e-02 1.67369327e+01 4.00000001e-02\n",
      "  4.00000000e-02]\n",
      " [4.00000000e-02 4.00000000e-02 8.11653895e+03 4.00000000e-02\n",
      "  4.00000000e-02 4.00000000e-02 4.00000000e-02 7.47360491e-01\n",
      "  4.00000000e-02 1.00824711e+04 4.00000000e-02 4.00000000e-02\n",
      "  5.81505647e+03 5.21588837e+04 7.05982810e+04 1.31186519e+05\n",
      "  4.00000000e-02 4.00000000e-02 1.86404726e+04 4.00000000e-02\n",
      "  4.00000000e-02 4.00000000e-02 4.00000000e-02 4.00000000e-02\n",
      "  4.00000000e-02]\n",
      " [4.00000000e-02 5.82563590e-02 4.00000000e-02 4.00000000e-02\n",
      "  6.16508436e-02 3.19882845e+00 4.00000000e-02 1.52557881e+03\n",
      "  1.92283645e+05 4.00000000e-02 4.00000000e-02 4.00000000e-02\n",
      "  4.00000000e-02 4.00000000e-02 4.00000000e-02 4.00000000e-02\n",
      "  4.00000000e-02 1.22982865e+00 4.00000000e-02 4.00000000e-02\n",
      "  5.22892690e-02 8.80149521e-02 3.88940801e+01 4.00000000e-02\n",
      "  4.00000000e-02]\n",
      " [4.00000000e-02 3.16308116e-01 1.43046063e+05 4.00000000e-02\n",
      "  4.00000000e-02 1.84610077e-01 9.68135977e-01 1.17308286e+04\n",
      "  4.00000000e-02 1.43908206e-01 4.00000000e-02 7.79878688e-02\n",
      "  4.00000000e-02 4.00000000e-02 4.00000000e-02 4.00000000e-02\n",
      "  7.84141639e-01 4.00000012e-02 4.48956133e-02 3.92894147e-01\n",
      "  4.00000000e-02 2.56593083e+01 2.50327445e+00 4.00017322e-02\n",
      "  4.00050201e-02]\n",
      " [4.00000000e-02 1.13159752e+03 1.24163299e+04 4.00000000e-02\n",
      "  4.00000000e-02 4.00000000e-02 4.00000000e-02 1.38124569e+04\n",
      "  1.85508629e+03 4.00000011e-02 1.61601114e+05 4.00000000e-02\n",
      "  4.00000000e-02 1.96223748e+02 1.71831156e+00 2.18580157e+01\n",
      "  4.00000000e-02 6.87682126e+03 1.78105665e+04 4.00000000e-02\n",
      "  4.00000000e-02 4.00000000e-02 2.51506558e+03 4.00000000e-02\n",
      "  4.00000000e-02]\n",
      " [4.00000000e-02 3.83233093e+05 4.00000000e-02 4.00000000e-02\n",
      "  4.00000000e-02 1.53550144e-01 3.53253081e-01 4.80480187e-01\n",
      "  4.00000000e-02 4.86865716e-01 4.00000000e-02 5.41611506e-01\n",
      "  4.00000000e-02 4.00000000e-02 4.00000000e-02 4.00000000e-02\n",
      "  4.00000000e-02 4.00000000e-02 4.00000000e-02 4.00000000e-02\n",
      "  4.00000000e-02 2.29741162e-01 1.69987149e+00 4.00000000e-02\n",
      "  4.00000000e-02]\n",
      " [4.00000000e-02 3.90107554e-01 4.00000000e-02 1.39434040e+05\n",
      "  4.00000000e-02 1.62170103e+00 3.52184546e-01 1.92965781e+00\n",
      "  4.00000000e-02 4.00066714e-02 4.00000000e-02 5.21596474e-02\n",
      "  4.00000000e-02 4.00000000e-02 4.00000000e-02 4.00000000e-02\n",
      "  7.54669818e-02 4.00000003e-02 4.00000000e-02 4.00000369e-02\n",
      "  4.87446433e-02 9.73104914e+00 2.80769315e+01 4.00000000e-02\n",
      "  1.03154376e+00]\n",
      " [4.00000000e-02 9.24352415e+00 4.00000000e-02 4.00000000e-02\n",
      "  4.00000000e-02 4.00000000e-02 3.82263748e-01 1.96688840e+01\n",
      "  4.00000000e-02 9.71693197e-01 4.00000000e-02 5.66189435e-02\n",
      "  4.00240297e-02 4.00000000e-02 4.00000000e-02 4.00000000e-02\n",
      "  4.00000000e-02 4.00000001e-02 4.00000023e-02 2.43899697e+05\n",
      "  9.98768777e-02 8.56633255e-02 8.63114424e-01 8.87419942e-02\n",
      "  4.00000000e-02]\n",
      " [4.00000000e-02 5.56129705e+01 4.00000000e-02 4.00000000e-02\n",
      "  4.00000000e-02 4.14549082e-02 4.00000000e-02 8.90298077e+01\n",
      "  4.00000000e-02 8.34394245e-02 4.00000008e-02 3.92784867e+05\n",
      "  4.00001531e-02 4.00000000e-02 4.00000000e-02 4.00000000e-02\n",
      "  4.00000002e-02 4.00527909e-02 4.25301359e-02 3.68984157e-01\n",
      "  2.96931061e-01 1.11428889e+00 1.76006180e+01 1.49950022e+00\n",
      "  4.00000000e-02]\n",
      " [4.00000000e-02 5.27691210e+01 4.00000000e-02 4.00000000e-02\n",
      "  4.00000000e-02 4.00000000e-02 1.99950529e-01 7.65533317e-01\n",
      "  4.00000000e-02 4.01398853e-02 4.00000000e-02 1.73555983e-01\n",
      "  4.00000000e-02 4.00000000e-02 4.00000000e-02 4.00000000e-02\n",
      "  4.23839714e-02 4.00000000e-02 4.00000000e-02 4.06672813e-02\n",
      "  2.72334692e+05 1.77700815e+00 4.01424275e-02 4.00000000e-02\n",
      "  4.00000000e-02]\n",
      " [4.00000000e-02 4.03758617e-02 4.00000000e-02 4.00000000e-02\n",
      "  4.00000000e-02 1.06562532e+02 4.00000000e-02 9.42168111e-01\n",
      "  4.00000000e-02 5.27327025e-02 4.00000000e-02 4.19943424e-01\n",
      "  3.68909775e-01 4.00000000e-02 4.00000000e-02 4.00000000e-02\n",
      "  4.00000000e-02 4.00000000e-02 4.00000301e-02 4.00000000e-02\n",
      "  4.00000000e-02 3.22078932e+05 7.54945323e-02 2.37302825e+00\n",
      "  4.00000000e-02]\n",
      " [4.00000000e-02 5.24693747e+00 4.00000000e-02 4.00000000e-02\n",
      "  4.00000000e-02 6.13790034e-02 2.13497825e-01 5.70915407e+00\n",
      "  4.00000000e-02 2.04157777e-01 4.00000000e-02 4.05162989e-01\n",
      "  4.00000000e-02 4.00000000e-02 4.00000000e-02 4.00000000e-02\n",
      "  4.00000022e-02 4.01427553e-02 2.12814622e-01 4.00000001e-02\n",
      "  4.00000000e-02 1.98371310e+00 1.89162120e+05 2.13844955e+05\n",
      "  4.00000000e-02]\n",
      " [2.88575040e+05 4.00000000e-02 4.00000000e-02 4.00000000e-02\n",
      "  4.00000000e-02 4.00000000e-02 4.00000000e-02 4.00000000e-02\n",
      "  4.00000000e-02 4.00000000e-02 4.00000000e-02 4.00000000e-02\n",
      "  4.00000000e-02 4.00000000e-02 4.00000000e-02 4.00000000e-02\n",
      "  4.00000000e-02 4.00000000e-02 4.00000000e-02 4.00000000e-02\n",
      "  4.00000000e-02 4.00000000e-02 4.00000000e-02 4.00000000e-02\n",
      "  4.00000000e-02]\n",
      " [4.00000000e-02 9.06420870e+03 3.26914349e+00 4.00000000e-02\n",
      "  4.00000000e-02 1.13198002e+02 4.64366878e+01 6.37174230e+03\n",
      "  4.02199624e-02 1.37442470e+01 4.71502047e+01 1.28964848e+04\n",
      "  3.36329431e+00 4.00000000e-02 4.00000000e-02 4.00000000e-02\n",
      "  5.07261464e+03 5.88951029e+00 2.40129390e+00 4.50202064e+03\n",
      "  8.17404459e+03 7.83077503e+00 1.08957700e+04 1.31516975e-01\n",
      "  4.00000000e-02]\n",
      " [4.00000000e-02 3.09221019e+00 4.00000000e-02 4.00000000e-02\n",
      "  4.00000000e-02 4.00000000e-02 4.00000000e-02 4.99641060e+01\n",
      "  9.37429346e-02 4.00000000e-02 4.00000737e-02 4.05522849e-02\n",
      "  4.00000000e-02 4.00000000e-02 4.00000000e-02 4.00000000e-02\n",
      "  2.23481940e+05 4.00000000e-02 6.30438195e-02 4.00000000e-02\n",
      "  4.00000000e-02 4.05003372e-02 1.12088868e-01 4.00000000e-02\n",
      "  4.00000000e-02]]\n"
     ]
    }
   ],
   "source": [
    "lda = LatentDirichletAllocation(n_components = 20, \n",
    "                                learning_method = \"batch\").fit(tf)\n",
    "# Topic-word matrix\n",
    "print(lda.components_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will be saving these features extracted using LDA into a pickle as they are not required for the network mining part to follow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_utc</th>\n",
       "      <th>ups</th>\n",
       "      <th>subreddit_id</th>\n",
       "      <th>link_id</th>\n",
       "      <th>name</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>id</th>\n",
       "      <th>author</th>\n",
       "      <th>body</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>...</th>\n",
       "      <th>question</th>\n",
       "      <th>realli</th>\n",
       "      <th>someth</th>\n",
       "      <th>thi</th>\n",
       "      <th>thing</th>\n",
       "      <th>think</th>\n",
       "      <th>time</th>\n",
       "      <th>want</th>\n",
       "      <th>work</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1430438400</td>\n",
       "      <td>3.0</td>\n",
       "      <td>t5_2qh1i</td>\n",
       "      <td>t3_34f9rh</td>\n",
       "      <td>t1_cqug90j</td>\n",
       "      <td>AskReddit</td>\n",
       "      <td>cqug90j</td>\n",
       "      <td>jesse9o3</td>\n",
       "      <td>No one has a European accent either  because i...</td>\n",
       "      <td>t1_cqug2sr</td>\n",
       "      <td>...</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>0.041138</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>0.041813</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>0.040000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1430438400</td>\n",
       "      <td>3.0</td>\n",
       "      <td>t5_2qh1i</td>\n",
       "      <td>t3_34fvry</td>\n",
       "      <td>t1_cqug90k</td>\n",
       "      <td>AskReddit</td>\n",
       "      <td>cqug90k</td>\n",
       "      <td>beltfedshooter</td>\n",
       "      <td>That the kid ..reminds me of Kevin.   so sad :-(</td>\n",
       "      <td>t3_34fvry</td>\n",
       "      <td>...</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.151136</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>0.040038</td>\n",
       "      <td>0.041170</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>24.421355</td>\n",
       "      <td>0.144514</td>\n",
       "      <td>24.834922</td>\n",
       "      <td>206708.614654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1430438400</td>\n",
       "      <td>5.0</td>\n",
       "      <td>t5_2qh1i</td>\n",
       "      <td>t3_34ffo5</td>\n",
       "      <td>t1_cqug90z</td>\n",
       "      <td>AskReddit</td>\n",
       "      <td>cqug90z</td>\n",
       "      <td>InterimFatGuy</td>\n",
       "      <td>NSFL</td>\n",
       "      <td>t1_cqu80zb</td>\n",
       "      <td>...</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>107364.016426</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>0.040000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1430438401</td>\n",
       "      <td>1.0</td>\n",
       "      <td>t5_2qh1i</td>\n",
       "      <td>t3_34aqsn</td>\n",
       "      <td>t1_cqug91c</td>\n",
       "      <td>AskReddit</td>\n",
       "      <td>cqug91c</td>\n",
       "      <td>JuanTutrego</td>\n",
       "      <td>I'm a guy and I had no idea this was a thing g...</td>\n",
       "      <td>t1_cqtdj4m</td>\n",
       "      <td>...</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.237241</td>\n",
       "      <td>0.485557</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>0.284080</td>\n",
       "      <td>0.388071</td>\n",
       "      <td>8311.343145</td>\n",
       "      <td>11712.427669</td>\n",
       "      <td>0.085683</td>\n",
       "      <td>10.473797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1430438401</td>\n",
       "      <td>101.0</td>\n",
       "      <td>t5_2qh1i</td>\n",
       "      <td>t3_34f9rh</td>\n",
       "      <td>t1_cqug91e</td>\n",
       "      <td>AskReddit</td>\n",
       "      <td>cqug91e</td>\n",
       "      <td>dcblackbelt</td>\n",
       "      <td>Mid twenties male rocking skinny jeans/pants, ...</td>\n",
       "      <td>t1_cquc4rc</td>\n",
       "      <td>...</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.040050</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>0.040303</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>0.055661</td>\n",
       "      <td>0.370109</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>0.040000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 45 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   created_utc    ups subreddit_id    link_id        name  subreddit       id  \\\n",
       "0   1430438400    3.0     t5_2qh1i  t3_34f9rh  t1_cqug90j  AskReddit  cqug90j   \n",
       "1   1430438400    3.0     t5_2qh1i  t3_34fvry  t1_cqug90k  AskReddit  cqug90k   \n",
       "2   1430438400    5.0     t5_2qh1i  t3_34ffo5  t1_cqug90z  AskReddit  cqug90z   \n",
       "3   1430438401    1.0     t5_2qh1i  t3_34aqsn  t1_cqug91c  AskReddit  cqug91c   \n",
       "4   1430438401  101.0     t5_2qh1i  t3_34f9rh  t1_cqug91e  AskReddit  cqug91e   \n",
       "\n",
       "           author                                               body  \\\n",
       "0        jesse9o3  No one has a European accent either  because i...   \n",
       "1  beltfedshooter   That the kid ..reminds me of Kevin.   so sad :-(   \n",
       "2   InterimFatGuy                                               NSFL   \n",
       "3     JuanTutrego  I'm a guy and I had no idea this was a thing g...   \n",
       "4     dcblackbelt  Mid twenties male rocking skinny jeans/pants, ...   \n",
       "\n",
       "    parent_id  ...  question    realli    someth            thi     thing  \\\n",
       "0  t1_cqug2sr  ...      0.04  0.040000  0.040000       0.040000  0.040000   \n",
       "1   t3_34fvry  ...      0.04  0.151136  0.040000       0.040038  0.041170   \n",
       "2  t1_cqu80zb  ...      0.04  0.040000  0.040000  107364.016426  0.040000   \n",
       "3  t1_cqtdj4m  ...      0.04  0.237241  0.485557       0.040000  0.284080   \n",
       "4  t1_cquc4rc  ...      0.04  0.040050  0.040000       0.040000  0.040303   \n",
       "\n",
       "      think         time          want       work           year  \n",
       "0  0.041138     0.040000      0.041813   0.040000       0.040000  \n",
       "1  0.040000    24.421355      0.144514  24.834922  206708.614654  \n",
       "2  0.040000     0.040000      0.040000   0.040000       0.040000  \n",
       "3  0.388071  8311.343145  11712.427669   0.085683      10.473797  \n",
       "4  0.040000     0.055661      0.370109   0.040000       0.040000  \n",
       "\n",
       "[5 rows x 45 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_1 = pd.DataFrame(lda.components_, columns=tf_vectorizer.get_feature_names())\n",
    "final_tokenized = pd.concat([df, df_1], axis=1)\n",
    "final_tokenized.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_preprocessed.to_pickle(\"preprocessed.pkl\")\n",
    "final_tokenized.to_pickle(\"lda_features.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing variables we will not need "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "149"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del text_preprocessed\n",
    "del final_tokenized\n",
    "del data\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Time based features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us first convert unix timestamp to datetime format in seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert created_utc to datetime\n",
    "df['time_convert'] = pd.to_datetime(df.created_utc, unit='s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining a function that would give us a dictionary containing the keys and the value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_dict(dic, x):\n",
    "    try:\n",
    "        return dic[x]\n",
    "    except:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Time since OP in seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtaining time of each comment\n",
    "comment_timing = pd.Series(df.time_convert.values, index = df.name).to_dict()\n",
    "df['time_diff_parent'] = df.parent_id.apply(lambda x: extract_dict(comment_timing, x))\n",
    "# Those that do not have a time, we assign them the publication time of the OP\n",
    "df.loc[df.time_diff_parent.isna(), 'time_since_parent'] = df.time_convert\n",
    "# Calculating the difference between the times\n",
    "df.time_diff_parent = df.time_convert - df.time_diff_parent\n",
    "df.time_diff_parent = df.time_diff_parent.apply(lambda x: x.total_seconds())\n",
    "del comment_timing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Hour of the comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding hour of the comment\n",
    "df['hour_of_comment'] = df.time_convert.apply(lambda x: x.hour)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Day of the comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Day of the comment\n",
    "df['day_of_comment'] = df.time_convert.dt.weekday"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Time_between_comments : Time between the OP's first comment and other comments in thread in seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dictionary with publication times for the first comment by thread.\n",
    "df_time_first_comment = df[['link_id', 'time_convert']].groupby(by=['link_id']).min().to_dict('index') \n",
    "# Time difference between the OP's first comment and other comments\n",
    "df['time_between_comments'] = df.time_convert - df.link_id.apply(lambda x: df_time_first_comment[x]['time_convert'])\n",
    "df.time_between_comments = df.time_between_comments.apply(lambda x: x.total_seconds())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Network Mining\n",
    "### C. Network based features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Reply to first comment : Earlier comments get more traction, and so do the replies to these comments \n",
    "Upon noticing that comments replying to the first comment to the post start with t3, we create the feature rep_first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['rep_first'] = df.parent_id.str.startswith('t3_').astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Thread popularity : calculating the length of the thread, or the number of unique authors in the link id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "auth_unique = pd.Series(df.author.values, index=df.link_id).groupby(by=['link_id']).count().to_dict()\n",
    "df['thread_pop'] = df.link_id.apply(lambda x: extract_dict(auth_unique, x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is where the network mining part actually starts. We need to get the depth of each thread : each comment (parent) would have its sub-comments (children). The job is to transform this into a node and edges relation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.DiGraph()\n",
    "# The link id can be used to generate the nodes as comments are linked through this link_id\n",
    "G.add_nodes_from(df.link_id, type = \"linkage\")\n",
    "# Now we get the comments that are linked - name is the unique name of the comment\n",
    "G.add_nodes_from(df.name, type = \"comment name\")\n",
    "# We then create the links between the comments which will ultimately give us the neighbour count\n",
    "G.add_edges_from(df[[\"parent_id\", \"name\"]].values, linktype = \"par\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given these characteristics, we can now get the depth of each comment and the number of neighbours or the comments attached to it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['comment_depth'] = [len(nx.ancestors(G, neighbour)) for neighbour in df['name']]\n",
    "df['count_nb'] = [G.degree[neighbour] for neighbour in df['name']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Centrality measures\n",
    "Let's now calculate the author-author interaction (undirected) : how much users interacted while commenting and how it possibly affects the score. \n",
    "To focus on the  relative importance of vertices within the network, we will calculate 3 of the 5 centrality metrics (the ones that took the shortest time to run), all implemented in networkx.\n",
    "\n",
    "    * degree centrality\n",
    "    * betweenness centrality\n",
    "    * eigenvector centrality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create two subsets\n",
    "* 1. Containing the author identified by his comment. \n",
    "* 2. Containing the author and the ID of the comment being replied to (parent_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_comment = df[['author', 'name']]\n",
    "author_parent_id = df[['author', 'parent_id']]\n",
    "# Je repère les noeuds à connecter\n",
    "merged = author_comment.merge(author_parent_id, left_on='name', right_on='parent_id', how='inner',\n",
    "                                  suffixes = ('_parent', '_child')) # On sait qui est le parent \n",
    "\n",
    "# Cleaning the entire merged df containing parents and children respectively\n",
    "merged = merged.groupby(['author_parent', 'author_child']).aggregate({'author_child':'count'})\n",
    "merged.columns = ['counts']\n",
    "merged.reset_index(level=0, drop=False, inplace=True)\n",
    "merged.reset_index(level=0, drop=False, inplace=True)\n",
    "\n",
    "# Finally ! We have the nodes that need to be connected ! \n",
    "nodes_at_last = merged.values\n",
    "    \n",
    "# Creating the graph for the users (Undirected)\n",
    "G1 = nx.Graph() # Undirected Simple type\n",
    "G1.add_weighted_edges_from(nodes_at_last)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The graph is not connected.\n"
     ]
    }
   ],
   "source": [
    "if nx.is_connected(G1) == False :\n",
    "    print('The graph is not connected.')\n",
    "else: \n",
    "    print('The graph is connected.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def centralities (df):\n",
    "    degree_centralities = []\n",
    "    betweenness_centralities = []\n",
    "    eigenvector_centralities = []\n",
    "  \n",
    "    # Here we are getting the degree centralities\n",
    "    degree_cent = nx.degree_centrality(G1)\n",
    "    \n",
    "    # Same for betweeness centrality\n",
    "    between_cent = nx.betweenness_centrality(G1, k = 60, weight = 'weight', normalized = True)\n",
    "    \n",
    "    # Lastly eigenvector centralities\n",
    "    eigen_cent = nx.eigenvector_centrality(G1, weight = 'weight')\n",
    "    \n",
    "    # Now we must append the features  \n",
    "    all_nodes = G1.nodes\n",
    "    # * collects all the positional arguments in a tuple\n",
    "    degree_cent_mean = int(pd.Series([*degree_cent.values()]).mean())\n",
    "    between_cent_mean = int(pd.Series([*between_cent.values()]).mean())\n",
    "    eigen_cent_mean = int(pd.Series([*eigen_cent.values()]).mean())\n",
    "    \n",
    "    for i in df.author :\n",
    "        if i in all_nodes:\n",
    "            degree_centralities.append(degree_cent[i])\n",
    "            betweenness_centralities.append(between_cent[i])\n",
    "            eigenvector_centralities.append(eigen_cent[i])\n",
    "            \n",
    "        else:\n",
    "            degree_centralities.append(degree_cent_mean)\n",
    "            betweenness_centralities.append(between_cent_mean)\n",
    "            eigenvector_centralities.append(eigen_cent_mean)\n",
    "            \n",
    "    df['degree_centrality'] = degree_centralities\n",
    "    df['betweenness_centrality'] = betweenness_centralities\n",
    "    df['eigenvector_centrality'] = eigenvector_centralities\n",
    "    \n",
    "    print(\"It took long... but move on\")\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It took long... but move on\n"
     ]
    }
   ],
   "source": [
    "df = centralities(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Score by parent_id\n",
    "\n",
    "Obtaining the score of comments by the parent_id or the score of the post to which the comments are a reply to. In case comments don't have a parent, I used median imputation (more robust)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_every_comment = pd.Series(df['ups'].values, index=df['name']).to_dict() # dictionary\n",
    "df['parent_score'] = df.parent_id.apply(lambda x: extract_dict(score_every_comment, x))\n",
    "# Median imputation\n",
    "median_value = df.parent_score.median()\n",
    "df.parent_score.fillna(median_value, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Number of comments made by the author "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_comment_count = pd.Series(df.name.values, index=df.author).groupby(by=['author']).count().to_dict()\n",
    "df['nb_com_author'] = df.author.apply(lambda x: extract_dict(author_comment_count, x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You have reached the end of the first notebook.\n",
    "We shall export these results to be used in the second half of the project - refer to notebook 2. Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle(\"dff.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "824129"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
